{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ephraimberkovitch/PrincetonNLPYiddish/blob/main/Spacy_Yiddish_Training_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "061ba15a"
      },
      "source": [
        "<img width=50% src=\"https://github.com/New-Languages-for-NLP/course-materials/raw/main/w2/using-inception-data/newnlp_notebook.png\" />\n",
        "\n",
        "For full documentation on this project, see [here](https://new-languages-for-nlp.github.io/course-materials/w2/using-inception-data/New%20Language%20Training.html)\n",
        " "
      ],
      "id": "061ba15a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psP7JdcRLR7n"
      },
      "source": [
        "# 1 Prepare the Notebook Environment"
      ],
      "id": "psP7JdcRLR7n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a0ba9e5a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title The Colab runtime comes with spaCy v2 and needs to be upgraded to v3.\n",
        "#@markdown This project uses the GPU by default, if you need to use just the CPU, just uncheck the box below.\n",
        "GPU = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Install spaCy v3 and libraries for GPUs and transformers\n",
        "!pip install spacy --upgrade\n",
        "if GPU:\n",
        "    !pip install 'spacy[transformers,cuda111]'\n",
        "#!pip install wandb spacy-huggingface-hub"
      ],
      "id": "a0ba9e5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfsEKZv6ErlG"
      },
      "source": [
        "The notebook will pull project files from your GitHub repository.  \n",
        "\n",
        "Note that you need to set the langugage (lang), treebank (same as the repo name), test_size and package name in the project.yml file in your repository.  "
      ],
      "id": "WfsEKZv6ErlG"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7c0bda8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "e7c2971f-1022-444c-bf21-7d6e6113bebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'newlang_project' from New-Languages-for-NLP/yiddish\u001b[0m\n",
            "/content/newlang_project\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/newlang_project\n",
            "\u001b[38;5;4mℹ Fetching 1 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/newlang_project/assets/yiddish\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title Enter your language's repository name. \n",
        "#@markdown If the repo is private, please check the \"private_repo\" box and include an [access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token).\n",
        "private_repo = False #@param {type:\"boolean\"}\n",
        "repo_name = \"yiddish\" #@param {type:\"string\"}\n",
        "branch = \"main\"\n",
        "\n",
        "\n",
        "!rm -rf /content/newlang_project\n",
        "!rm -rf $repo_name\n",
        "if private_repo:\n",
        "    git_access_token = \"\" #@param {type:\"string\"}\n",
        "    git_url = f\"https://{git_access_token}@github.com/New-Languages-for-NLP/{repo_name}/\"\n",
        "    !git clone $git_url  -b $branch\n",
        "    !cp -r ./$repo_name/newlang_project .  \n",
        "    !mkdir newlang_project/assets/\n",
        "    !mkdir newlang_project/configs/\n",
        "    #!mkdir newlang_project/corpus/\n",
        "    !mkdir newlang_project/metrics/\n",
        "    !mkdir newlang_project/packages/\n",
        "    !mkdir newlang_project/training/\n",
        "    !mkdir newlang_project/assets/$repo_name\n",
        "    !cp -r ./$repo_name/* newlang_project/assets/$repo_name/\n",
        "    !rm -rf ./$repo_name\n",
        "else:\n",
        "    !python -m spacy project clone newlang_project --repo https://github.com/New-Languages-for-NLP/$repo_name --branch $branch\n",
        "    !python -m spacy project assets /content/newlang_project"
      ],
      "id": "7c0bda8e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4dc13741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222895a8-c4bd-4099-a65a-9a4eda421a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== install ==================================\u001b[0m\n",
            "Running command: rm -rf lang\n",
            "Running command: mkdir lang\n",
            "Running command: mkdir lang/yi\n",
            "Running command: cp -r assets/yiddish/2_new_language_object/ lang/yi/yi\n",
            "Running command: mv lang/yi/yi/setup.py lang/yi/\n",
            "Running command: /usr/bin/python3 -m pip install -e lang/yi\n",
            "Obtaining file:///content/newlang_project/lang/yi\n",
            "Installing collected packages: yi\n",
            "  Running setup.py develop for yi\n",
            "Successfully installed yi-0.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install the custom language object from Cadet \n",
        "!python -m spacy project run install /content/newlang_project"
      ],
      "id": "4dc13741"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Pretraining**](https://spacy.io/usage/embeddings-transformers#pretraining) (optional)\n",
        "\n",
        "With pretraining, spaCy will learn initial token embeddings from your raw text files.  This can lead to significant improvement when you don't have a lot of data. \n",
        "*  \n",
        "* It will load the raw text files (.txt) from your `0_original_texts` folder in GitHub"
      ],
      "metadata": {
        "id": "6erLtVVNA2VH"
      },
      "id": "6erLtVVNA2VH"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy project run prep-rawtext /content/newlang_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_9jAy8CPfBL",
        "outputId": "20268cba-2cfd-45ac-bd83-bfcb568bb5e1"
      },
      "id": "Z_9jAy8CPfBL",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[38;5;1m✘ Can't find command or workflow 'prep-rawtext' in project.yml\u001b[0m\n",
            "Available commands: install, convert, split, debug, train, evaluate, package,\n",
            "document. Available workflows: all\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy pretrain /content/newlang_project/configs/config_pretrain.cfg ./pretrain --gpu-id 0"
      ],
      "metadata": {
        "id": "dL-cXmmqPcv_",
        "outputId": "edc324bf-2125-4544-8d55-f6ed43f45d7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dL-cXmmqPcv_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: python -m spacy pretrain \n",
            "           [OPTIONS] CONFIG_PATH OUTPUT_DIR\n",
            "Try 'python -m spacy pretrain --help' for help.\n",
            "\n",
            "Error: Invalid value for 'CONFIG_PATH': File '/content/newlang_project/configs/config_pretrain.cfg' does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU_AqRK6LZrF"
      },
      "source": [
        "# 2 Prepare the Data for Training"
      ],
      "id": "qU_AqRK6LZrF"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPjD0SN6iAvc",
        "outputId": "e3cd4548-66c3-460e-d9f1-957925d2ada2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n"
          ]
        }
      ],
      "source": [
        "#@title (optional) cell to correct a problem when your tokens have no pos value\n",
        "%%writefile /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n",
        "import re\n",
        "\n",
        "from .conll_ner_to_docs import n_sents_info\n",
        "from ...training import iob_to_biluo, biluo_tags_to_spans\n",
        "from ...tokens import Doc, Token, Span\n",
        "from ...vocab import Vocab\n",
        "from wasabi import Printer\n",
        "\n",
        "\n",
        "def conllu_to_docs(\n",
        "    input_data,\n",
        "    n_sents=10,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "    merge_subtokens=False,\n",
        "    no_print=False,\n",
        "    **_\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert conllu files into JSON format for use with train cli.\n",
        "    append_morphology parameter enables appending morphology to tags, which is\n",
        "    useful for languages such as Spanish, where UD tags are not so rich.\n",
        "\n",
        "    Extract NER tags if available and convert them so that they follow\n",
        "    BILUO and the Wikipedia scheme\n",
        "    \"\"\"\n",
        "    MISC_NER_PATTERN = \"^((?:name|NE)=)?([BILU])-([A-Z_]+)|O$\"\n",
        "    msg = Printer(no_print=no_print)\n",
        "    n_sents_info(msg, n_sents)\n",
        "    sent_docs = read_conllx(\n",
        "        input_data,\n",
        "        append_morphology=append_morphology,\n",
        "        ner_tag_pattern=MISC_NER_PATTERN,\n",
        "        ner_map=ner_map,\n",
        "        merge_subtokens=merge_subtokens,\n",
        "    )\n",
        "    sent_docs_to_merge = []\n",
        "    for sent_doc in sent_docs:\n",
        "        sent_docs_to_merge.append(sent_doc)\n",
        "        if len(sent_docs_to_merge) % n_sents == 0:\n",
        "            yield Doc.from_docs(sent_docs_to_merge)\n",
        "            sent_docs_to_merge = []\n",
        "    if sent_docs_to_merge:\n",
        "        yield Doc.from_docs(sent_docs_to_merge)\n",
        "\n",
        "\n",
        "def has_ner(input_data, ner_tag_pattern):\n",
        "    \"\"\"\n",
        "    Check the MISC column for NER tags.\n",
        "    \"\"\"\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            for line in lines:\n",
        "                parts = line.split(\"\\t\")\n",
        "                id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "                for misc_part in misc.split(\"|\"):\n",
        "                    if re.match(ner_tag_pattern, misc_part):\n",
        "                        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def read_conllx(\n",
        "    input_data,\n",
        "    append_morphology=False,\n",
        "    merge_subtokens=False,\n",
        "    ner_tag_pattern=\"\",\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Yield docs, one for each sentence\"\"\"\n",
        "    vocab = Vocab()  # need vocab to make a minimal Doc\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            doc = conllu_sentence_to_doc(\n",
        "                vocab,\n",
        "                lines,\n",
        "                ner_tag_pattern,\n",
        "                merge_subtokens=merge_subtokens,\n",
        "                append_morphology=append_morphology,\n",
        "                ner_map=ner_map,\n",
        "            )\n",
        "            yield doc\n",
        "\n",
        "\n",
        "def get_entities(lines, tag_pattern, ner_map=None):\n",
        "    \"\"\"Find entities in the MISC column according to the pattern and map to\n",
        "    final entity type with `ner_map` if mapping present. Entity tag is 'O' if\n",
        "    the pattern is not matched.\n",
        "\n",
        "    lines (str): CONLL-U lines for one sentences\n",
        "    tag_pattern (str): Regex pattern for entity tag\n",
        "    ner_map (dict): Map old NER tag names to new ones, '' maps to O.\n",
        "    RETURNS (list): List of BILUO entity tags\n",
        "    \"\"\"\n",
        "    miscs = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_ or \".\" in id_:\n",
        "            continue\n",
        "        miscs.append(misc)\n",
        "\n",
        "    iob = []\n",
        "    for misc in miscs:\n",
        "        iob_tag = \"O\"\n",
        "        for misc_part in misc.split(\"|\"):\n",
        "            tag_match = re.match(tag_pattern, misc_part)\n",
        "            if tag_match:\n",
        "                prefix = tag_match.group(2)\n",
        "                suffix = tag_match.group(3)\n",
        "                if prefix and suffix:\n",
        "                    iob_tag = prefix + \"-\" + suffix\n",
        "                    if ner_map:\n",
        "                        suffix = ner_map.get(suffix, suffix)\n",
        "                        if suffix == \"\":\n",
        "                            iob_tag = \"O\"\n",
        "                        else:\n",
        "                            iob_tag = prefix + \"-\" + suffix\n",
        "                break\n",
        "        iob.append(iob_tag)\n",
        "    return iob_to_biluo(iob)\n",
        "\n",
        "\n",
        "def conllu_sentence_to_doc(\n",
        "    vocab,\n",
        "    lines,\n",
        "    ner_tag_pattern,\n",
        "    merge_subtokens=False,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Create an Example from the lines for one CoNLL-U sentence, merging\n",
        "    subtokens and appending morphology to tags if required.\n",
        "\n",
        "    lines (str): The non-comment lines for a CoNLL-U sentence\n",
        "    ner_tag_pattern (str): The regex pattern for matching NER in MISC col\n",
        "    RETURNS (Example): An example containing the annotation\n",
        "    \"\"\"\n",
        "    # create a Doc with each subtoken as its own token\n",
        "    # if merging subtokens, each subtoken orth is the merged subtoken form\n",
        "    if not Token.has_extension(\"merged_orth\"):\n",
        "        Token.set_extension(\"merged_orth\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_lemma\"):\n",
        "        Token.set_extension(\"merged_lemma\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_morph\"):\n",
        "        Token.set_extension(\"merged_morph\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_spaceafter\"):\n",
        "        Token.set_extension(\"merged_spaceafter\", default=\"\")\n",
        "    words, spaces, tags, poses, morphs, lemmas = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    subtok_word = \"\"\n",
        "    in_subtok = False\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \".\" in id_:\n",
        "            continue\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "            subtok_word = word\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_spaceafter = \"SpaceAfter=No\" not in misc\n",
        "            continue\n",
        "        if merge_subtokens and in_subtok:\n",
        "            words.append(subtok_word)\n",
        "        else:\n",
        "            words.append(word)\n",
        "        if in_subtok:\n",
        "            if id_ == subtok_end:\n",
        "                spaces.append(subtok_spaceafter)\n",
        "            else:\n",
        "                spaces.append(False)\n",
        "        elif \"SpaceAfter=No\" in misc:\n",
        "            spaces.append(False)\n",
        "        else:\n",
        "            spaces.append(True)\n",
        "        if in_subtok and id_ == subtok_end:\n",
        "            subtok_word = \"\"\n",
        "            in_subtok = False\n",
        "        id_ = int(id_) - 1\n",
        "        head = (int(head) - 1) if head not in (\"0\", \"_\") else id_\n",
        "        tag = pos if tag == \"_\" else tag\n",
        "        morph = morph if morph != \"_\" else \"\"\n",
        "        dep = \"ROOT\" if dep == \"root\" else dep\n",
        "        lemmas.append(lemma)\n",
        "        if pos == \"_\":\n",
        "            pos = \"\"\n",
        "        poses.append(pos)\n",
        "        tags.append(tag)\n",
        "        morphs.append(morph)\n",
        "        heads.append(head)\n",
        "        deps.append(dep)\n",
        "\n",
        "    doc = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        lemmas=lemmas,\n",
        "        morphs=morphs,\n",
        "        heads=heads,\n",
        "    )\n",
        "    for i in range(len(doc)):\n",
        "        doc[i]._.merged_orth = words[i]\n",
        "        doc[i]._.merged_morph = morphs[i]\n",
        "        doc[i]._.merged_lemma = lemmas[i]\n",
        "        doc[i]._.merged_spaceafter = spaces[i]\n",
        "    ents = get_entities(lines, ner_tag_pattern, ner_map)\n",
        "    doc.ents = biluo_tags_to_spans(doc, ents)\n",
        "\n",
        "    if merge_subtokens:\n",
        "        doc = merge_conllu_subtokens(lines, doc)\n",
        "\n",
        "    # create final Doc from custom Doc annotation\n",
        "    words, spaces, tags, morphs, lemmas, poses = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    for i, t in enumerate(doc):\n",
        "        words.append(t._.merged_orth)\n",
        "        lemmas.append(t._.merged_lemma)\n",
        "        spaces.append(t._.merged_spaceafter)\n",
        "        morphs.append(t._.merged_morph)\n",
        "        if append_morphology and t._.merged_morph:\n",
        "            tags.append(t.tag_ + \"__\" + t._.merged_morph)\n",
        "        else:\n",
        "            tags.append(t.tag_)\n",
        "        poses.append(t.pos_)\n",
        "        heads.append(t.head.i)\n",
        "        deps.append(t.dep_)\n",
        "\n",
        "    doc_x = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        morphs=morphs,\n",
        "        lemmas=lemmas,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        heads=heads,\n",
        "    )\n",
        "    doc_x.ents = [Span(doc_x, ent.start, ent.end, label=ent.label) for ent in doc.ents]\n",
        "\n",
        "    return doc_x\n",
        "\n",
        "\n",
        "def merge_conllu_subtokens(lines, doc):\n",
        "    # identify and process all subtoken spans to prepare attrs for merging\n",
        "    subtok_spans = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_:\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_span = doc[int(subtok_start) - 1 : int(subtok_end)]\n",
        "            subtok_spans.append(subtok_span)\n",
        "            # create merged tag, morph, and lemma values\n",
        "            tags = []\n",
        "            morphs = {}\n",
        "            lemmas = []\n",
        "            for token in subtok_span:\n",
        "                tags.append(token.tag_)\n",
        "                lemmas.append(token.lemma_)\n",
        "                if token._.merged_morph:\n",
        "                    for feature in token._.merged_morph.split(\"|\"):\n",
        "                        field, values = feature.split(\"=\", 1)\n",
        "                        if field not in morphs:\n",
        "                            morphs[field] = set()\n",
        "                        for value in values.split(\",\"):\n",
        "                            morphs[field].add(value)\n",
        "            # create merged features for each morph field\n",
        "            for field, values in morphs.items():\n",
        "                morphs[field] = field + \"=\" + \",\".join(sorted(values))\n",
        "            # set the same attrs on all subtok tokens so that whatever head the\n",
        "            # retokenizer chooses, the final attrs are available on that token\n",
        "            for token in subtok_span:\n",
        "                token._.merged_orth = token.orth_\n",
        "                token._.merged_lemma = \" \".join(lemmas)\n",
        "                token.tag_ = \"_\".join(tags)\n",
        "                token._.merged_morph = \"|\".join(sorted(morphs.values()))\n",
        "                token._.merged_spaceafter = (\n",
        "                    True if subtok_span[-1].whitespace_ else False\n",
        "                )\n",
        "\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for span in subtok_spans:\n",
        "            retokenizer.merge(span)\n",
        "\n",
        "    return doc"
      ],
      "id": "HPjD0SN6iAvc"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "563fdc94",
        "outputId": "e1a801af-ad94-4705-ccba-dd21f941f2a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== convert ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/convert.py assets/yiddish/3_inception_export 10 yi\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_360_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_307_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_362_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1054.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_38_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_355_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_315_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_368_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_57_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (3 documents):\n",
            "corpus/conllu/Forverts_30_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_377_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_325_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1295.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_152_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_363_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_375_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_353_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_358_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_342_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (3 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1051.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_33_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_341_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_310_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_39_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (8 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1290.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '15' with value '27' (equivalent to relative head index: '27'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_371_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_37_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1060.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (5 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1053.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_354_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_359_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '22' (equivalent to relative head index: '22'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_370_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_351_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_312_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (6 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1296.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_34_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_350_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_372_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '33' with value '4' (equivalent to relative head index: '4'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_338_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_331_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_336_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_125_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_344_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_374_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_329_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_31_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (6 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1052.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_79_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_326_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4 documents):\n",
            "corpus/conllu/processed.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_35_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '33' (equivalent to relative head index: '33'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_321_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_319_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_345_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_365_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_318_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '31' (equivalent to relative head index: '31'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '64' (equivalent to relative head index: '64'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_357_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '9' with value '22' (equivalent to relative head index: '22'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_305_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_343_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_313_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_314_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_303_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (23 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1298.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (3 documents):\n",
            "corpus/conllu/Forverts_380_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '32' with value '36' (equivalent to relative head index: '36'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_304_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_378_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_356_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_337_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_332_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_323_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (4 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1291.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_327_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (7 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1293.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '6' with value '21' (equivalent to relative head index: '21'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1056.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_347_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '71' (equivalent to relative head index: '71'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_322_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_330_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (22 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1297.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_187_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_339_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_311_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_14_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_376_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_349_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_328_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_32_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_306_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_348_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_36_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_301_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '42' (equivalent to relative head index: '42'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_379_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_324_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (10 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1292.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_335_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (8 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1059.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 89, in convert_cli\n",
            "    msg=msg,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/convert.py\", line 140, in convert\n",
            "    db = DocBin(docs=docs, store_user_data=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\", line 86, in __init__\n",
            "    for doc in docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 38, in conllu_to_docs\n",
            "    for sent_doc in sent_docs:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 85, in read_conllx\n",
            "    ner_map=ner_map,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\", line 211, in conllu_sentence_to_doc\n",
            "    heads=heads,\n",
            "  File \"spacy/tokens/doc.pyx\", line 378, in spacy.tokens.doc.Doc.__init__\n",
            "  File \"spacy/tokens/doc.pyx\", line 1073, in spacy.tokens.doc.Doc.from_array\n",
            "ValueError: [E190] Token head out of range in `Doc.from_array()` for token index '0' with value '52' (equivalent to relative head index: '52'). The head indices should be relative to the current token index rather than absolute indices in the array.\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_369_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (2 documents):\n",
            "corpus/conllu/Forverts_340_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_309_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_364_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_317_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1057.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_373_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_316_allPs.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (14 documents):\n",
            "corpus/conllu/BirobidzhannerShtern_1058.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents):\n",
            "corpus/conllu/Forverts_352_allPs.spacy\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Convert the conllu files from inception to spaCy binary format\n",
        "# Read the conll files with ner data and as ents to spaCy docs \n",
        "!python -m spacy project run convert /content/newlang_project"
      ],
      "id": "563fdc94"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9519c858",
        "outputId": "0721bb64-c2a1-4483-9e87-fa0f6fcdc3d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== split ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/split.py 0.2 11 yi\n",
            "🚂 Created 194 training docs\n",
            "😊 Created 39 validation docs\n",
            "🧪  Created 10 test docs\n"
          ]
        }
      ],
      "source": [
        "# test/train split \n",
        "!python -m spacy project run split /content/newlang_project "
      ],
      "id": "9519c858"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4feefe6f",
        "outputId": "a912a87d-08bc-4c02-8580-6c38ae9dcf04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== debug ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy debug data configs/config.cfg\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: yi\n",
            "Training pipeline: tok2vec, tagger, parser, ner\n",
            "194 training docs\n",
            "39 evaluation docs\n",
            "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (194)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 45544 total word(s) in the data (8247 unique)\u001b[0m\n",
            "\u001b[38;5;3m⚠ 7 misaligned tokens in the training data\u001b[0m\n",
            "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
            "\u001b[1m\n",
            "========================== Named Entity Recognition ==========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 0 label(s)\u001b[0m\n",
            "0 missing value(s) (tokens with '-' label)\n",
            "\u001b[38;5;2m✔ Good amount of examples for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities crossing sentence boundaries\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Part-of-speech Tagging ===========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 56 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Some model labels are not present in the train data. The model\n",
            "performance may be degraded for these labels after training: 'XY'.\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Dependency Parsing =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 45032 sentence(s) with an average length of 1.0 words.\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 2 nonprojective train sentence(s)\u001b[0m\n",
            "\u001b[38;5;4mℹ 33 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;4mℹ 41 label(s) in projectivized train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'aux' (16)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'iobj' (6)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'flat' (16)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'det:poss' (15)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'amod' (20)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'xcomp' (8)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'cc' (12)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'advcl' (6)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'conj' (19)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'ccomp' (3)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'appos' (9)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'cop' (9)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'compound:prt' (1)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'compound' (1)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'dep' (1)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'nummod' (1)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'acl' (5)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'nsubj:pass' (2)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'aux:pass' (3)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'parataxis' (2)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'expl' (1)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'csubj' (1)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for 8 label(s) in the projectivized dependency\n",
            "trees used for training. You may want to projectivize labels such as punct\n",
            "before training in order to improve parser performance.\u001b[0m\n",
            "\u001b[38;5;3m⚠ Multiple root labels (_, ROOT) found in training data. spaCy's parser\n",
            "uses a single root label ROOT so this distinction will not be available.\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 7 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 30 warnings\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Debug the data\n",
        "!python -m spacy project run debug /content/newlang_project "
      ],
      "id": "4feefe6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "151-cj1dLgAD"
      },
      "source": [
        "# 3 Model Training "
      ],
      "id": "151-cj1dLgAD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKW2QGPTJ_CZ"
      },
      "source": [
        "If your project file uses Weights and Biases to monitor model training, you'll need to create an account at [wandb.ai](https://wandb.ai/site) and get an API key.  "
      ],
      "id": "KKW2QGPTJ_CZ"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "38b490a4",
        "outputId": "3e8d699d-eae6-486b-95c5-78fb8a27fe63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/config.cfg --output training/yiddish --gpu-id 0 --nlp.lang=yi\n",
            "\u001b[38;5;2m✔ Created output directory: training/yiddish\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/yiddish\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-04-15 06:25:50,548] [INFO] Set up nlp object from config\n",
            "[2022-04-15 06:25:50,563] [INFO] Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\n",
            "[2022-04-15 06:25:50,570] [INFO] Created vocabulary\n",
            "[2022-04-15 06:25:50,571] [INFO] Finished initializing nlp object\n",
            "[2022-04-15 06:27:07,832] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'parser', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  LOSS NER  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  -----------  -----------  --------  -------  -------  -------  -------  ------  ------  ------  ------\n",
            "  0       0          0.00       144.42         6.38     73.50    42.17    98.83     0.09    99.28    0.00    0.00    0.00    0.31\n",
            "  1     200        292.93     11096.55     32072.69    390.47    94.41    98.83     0.09    99.28    0.00    0.00    0.00    0.48\n",
            "  2     400        416.64      2181.00     27348.90      0.04    95.82    98.84     0.09    99.30    0.00    0.00    0.00    0.48\n",
            "  3     600       1554.70      1755.79     30120.41      0.00    96.71    98.83     0.09    99.28    0.00    0.00    0.00    0.49\n",
            "  4     800         85.47       238.37     25878.40      0.00    96.30    98.83     0.09    99.28    0.00    0.00    0.00    0.49\n",
            "  5    1000       1005.42       997.31     29669.32      0.00    96.97    98.83     0.09    99.28    0.00    0.00    0.00    0.49\n",
            "  6    1200        322.85       311.31     28000.67      0.00    96.54    98.83     0.09    99.28    0.00    0.00    0.00    0.49\n",
            "  7    1400        278.47       356.92     27353.95      0.00    97.02    98.96     0.09    99.55    0.00    0.00    0.00    0.49\n",
            "  9    1600       1907.66       548.22     32462.90      0.00    97.17    99.16     0.02    99.81    0.00    0.00    0.00    0.49\n",
            " 10    1800       1836.51       293.81     30088.94      0.00    97.02    98.83     0.09    99.28    0.00    0.00    0.00    0.49\n",
            " 11    2000        414.72       293.16     36525.42      0.00    97.30    99.03     0.07    99.69    0.00    0.00    0.00    0.49\n",
            " 13    2200        723.15       197.28     39615.38      0.00    97.23    98.93     0.10    99.34    0.00    0.00    0.00    0.49\n",
            " 15    2400        647.57       149.49     46874.63      0.00    96.84    98.98     0.13    99.49    0.00    0.00    0.00    0.49\n",
            " 17    2600        743.56       152.05     48977.41      0.00    97.22    98.85     0.10    99.29    0.00    0.00    0.00    0.49\n",
            " 19    2800       3498.69       155.01     60587.97      0.00    97.10    98.81     0.06    99.49    0.00    0.00    0.00    0.49\n",
            " 22    3000       2060.21       107.32     71103.48      0.00    97.14    99.04     0.14    99.65    0.00    0.00    0.00    0.49\n",
            " 26    3200       4461.11        93.75     81651.06      0.00    97.12    99.15     0.10    99.84    0.00    0.00    0.00    0.49\n",
            " 29    3400       3217.58        72.78     91071.08      0.00    97.13    99.14     0.13    99.80    0.00    0.00    0.00    0.49\n",
            " 33    3600       1514.52        74.65     88542.86      0.00    97.26    99.26     0.15    99.75    0.00    0.00    0.00    0.49\n",
            " 36    3800       2035.36        71.27     90930.70      0.00    97.31    99.22     0.14    99.82    0.00    0.00    0.00    0.49\n",
            " 40    4000       3155.55        60.13     85472.43      0.00    97.08    99.09     0.17    99.80    0.00    0.00    0.00    0.49\n",
            " 44    4200       2315.84        47.95     89625.83      0.00    97.17    99.22     0.14    99.85    0.00    0.00    0.00    0.49\n",
            " 47    4400       1312.00        39.39     86333.11      0.00    97.09    99.21     0.17    99.84    0.00    0.00    0.00    0.49\n",
            " 51    4600       2068.66        50.49     86529.84      0.00    97.30    99.29     0.20    99.76    0.00    0.00    0.00    0.49\n",
            " 54    4800       4077.70        41.37     89779.51      0.00    97.04    99.12     0.17    99.68    0.00    0.00    0.00    0.49\n",
            " 58    5000       1646.63        33.73     82341.03      0.00    97.30    99.24     0.20    99.85    0.00    0.00    0.00    0.49\n",
            " 61    5200       2120.68        36.29     85569.79      0.00    97.27    99.28     0.18    99.82    0.00    0.00    0.00    0.49\n",
            " 65    5400       1497.82        35.97     87569.55      0.00    97.29    99.23     0.13    99.89    0.00    0.00    0.00    0.49\n",
            " 69    5600       2311.22        45.97     87461.91      0.00    97.23    99.24     0.19    99.79    0.00    0.00    0.00    0.49\n",
            " 72    5800        820.39        19.33     84991.25      0.00    97.22    99.33     0.21    99.91    0.00    0.00    0.00    0.49\n",
            " 76    6000       2507.99        32.63     86318.99      0.00    97.14    99.19     0.18    99.72    0.00    0.00    0.00    0.49\n",
            " 79    6200       1098.21        21.63     84314.69      0.00    96.99    99.29     0.18    99.80    0.00    0.00    0.00    0.49\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training/yiddish/model-last\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "!python -m spacy project run train /content/newlang_project "
      ],
      "id": "38b490a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynXr8vlXqCxv"
      },
      "source": [
        "If you get `ValueError: Could not find gold transition - see logs above.`  \n",
        "You may not have sufficent data to train on: https://github.com/explosion/spaCy/discussions/7282"
      ],
      "id": "ynXr8vlXqCxv"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "018362d8",
        "outputId": "8a8fb88c-404b-4b64-bd35-1c215cd57c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy evaluate ./training/yiddish/model-best ./corpus/converted/test.spacy --output ./metrics/yiddish.json --gpu-id 0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK      100.00\n",
            "TAG      98.07 \n",
            "UAS      100.00\n",
            "LAS      0.00  \n",
            "NER P    -     \n",
            "NER R    -     \n",
            "NER F    -     \n",
            "SENT P   100.00\n",
            "SENT R   100.00\n",
            "SENT F   100.00\n",
            "SPEED    9755  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== LAS (per type) ===============================\u001b[0m\n",
            "\n",
            "          P      R      F\n",
            "_      0.00   0.00   0.00\n",
            "root   0.00   0.00   0.00\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to metrics/yiddish.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model using the test data\n",
        "!python -m spacy project run evaluate /content/newlang_project "
      ],
      "id": "018362d8"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gg1sLlVrgiyu",
        "outputId": "12ea225a-df28-4cdc-f33b-3aa4d03681f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.cfg  meta.json  ner  parser  tagger  tok2vec  tokenizer\tvocab\n"
          ]
        }
      ],
      "source": [
        "# Find the path for your meta.json file\n",
        "# You'll need to add newlang_project/ +  the path from the training step just after \"✔ Saved pipeline to output directory\"\n",
        "!ls /content/newlang_project/training/yiddish/model-last"
      ],
      "id": "gg1sLlVrgiyu"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3zGCTURr9JE6"
      },
      "outputs": [],
      "source": [
        "#Update meta.json\n",
        "import spacy \n",
        "import srsly \n",
        "\n",
        "# Change path to match that from the training cell where it says \"✔ Saved pipeline to output directory\"\n",
        "meta_path = \"/content/newlang_project/training/yiddish/model-last/meta.json\"\n",
        "\n",
        "# Replace values below for your project\n",
        "my_meta = { \n",
        "    \"lang\":\"yi\",\n",
        "    \"name\":\"yiddish_sm\",\n",
        "    \"version\":\"0.0.1\",\n",
        "    \"description\":\"Yiddish pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, lemmatizer.\",\n",
        "    \"author\":\"New Languages for NLP\",\n",
        "    \"email\":\"newnlp@princeton.edu\",\n",
        "    \"url\":\"https://newnlp.princeton.edu\",\n",
        "    \"license\":\"MIT\", \n",
        "    }\n",
        "meta = spacy.util.load_meta(meta_path)\n",
        "meta.update(my_meta)\n",
        "srsly.write_json(meta_path, meta)"
      ],
      "id": "3zGCTURr9JE6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM309FhNVAeb"
      },
      "source": [
        "### Download the trained model to your computer.\n"
      ],
      "id": "JM309FhNVAeb"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8e1d6f36",
        "outputId": "3d5d80ee-85ef-465c-a8ba-ed2b08d53ab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Building package artifacts: sdist\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "/content/newlang_project/training/yiddish/model-last/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package directory 'yi_yiddish_sm-0.0.1'\u001b[0m\n",
            "newlang_project/export/yi_yiddish_sm-0.0.1\n",
            "running sdist\n",
            "running egg_info\n",
            "creating yi_yiddish_sm.egg-info\n",
            "writing yi_yiddish_sm.egg-info/PKG-INFO\n",
            "writing dependency_links to yi_yiddish_sm.egg-info/dependency_links.txt\n",
            "writing entry points to yi_yiddish_sm.egg-info/entry_points.txt\n",
            "writing requirements to yi_yiddish_sm.egg-info/requires.txt\n",
            "writing top-level names to yi_yiddish_sm.egg-info/top_level.txt\n",
            "writing manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "reading manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "running check\n",
            "creating yi_yiddish_sm-0.0.1\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying files to yi_yiddish_sm-0.0.1...\n",
            "copying MANIFEST.in -> yi_yiddish_sm-0.0.1\n",
            "copying README.md -> yi_yiddish_sm-0.0.1\n",
            "copying meta.json -> yi_yiddish_sm-0.0.1\n",
            "copying setup.py -> yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/__init__.py -> yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "copying yi_yiddish_sm/meta.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "copying yi_yiddish_sm.egg-info/PKG-INFO -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/SOURCES.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/dependency_links.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/entry_points.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/not-zip-safe -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/requires.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/top_level.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/README.md -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/config.cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/meta.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tokenizer -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/moves -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/moves -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/key2row -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/lookups.bin -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/strings.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/vectors -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/vectors.cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "Writing yi_yiddish_sm-0.0.1/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'yi_yiddish_sm-0.0.1' (and everything under it)\n",
            "\u001b[38;5;2m✔ Successfully created zipped Python package\u001b[0m\n",
            "newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Save the model to disk in a format that can be easily  downloaded and re-used.\n",
        "!python -m spacy package /content/newlang_project/training/yiddish/model-last newlang_project/export "
      ],
      "id": "8e1d6f36"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8d32abf2",
        "outputId": "e4f5c6ec-4f51-45b7-e696-eeb8978cc5e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_56ec4f0e-6bbf-4c15-bca5-b56bccfc5000\", \"yi_yiddish_sm-0.0.1.tar.gz\", 7825983)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "# replace with the path in the previous cell under \"✔ Successfully created zipped Python package\"\n",
        "files.download('newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz')\n",
        "\n",
        "# once on your computer, you can pip install yi_yiddish_sm-0.0.1.tar.gz\n",
        "# Be sure to add the file to the 4_trained_models folder in GitHub"
      ],
      "id": "8d32abf2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using your trained model "
      ],
      "metadata": {
        "id": "xVpfmRBBVA-R"
      },
      "id": "xVpfmRBBVA-R"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the model as a module \n",
        "!pip install newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz\n",
        "# If you're using Jupyter or Colab, you may need to restart the runtime after installation of your model for it to be available. \n"
      ],
      "metadata": {
        "id": "vWJ-wz_5VCsZ",
        "outputId": "bdc02661-e143-4fa5-a1a1-d825fe594866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vWJ-wz_5VCsZ",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz\n",
            "Requirement already satisfied: spacy<3.3.0,>=3.2.4 in /usr/local/lib/python3.7/dist-packages (from yi-yiddish-sm==0.0.1) (3.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (0.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (8.0.15)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (1.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (3.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (7.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (1.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (1.21.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (0.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (3.10.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (4.64.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (2.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.4->yi-yiddish-sm==0.0.1) (2.0.1)\n",
            "Building wheels for collected packages: yi-yiddish-sm\n",
            "  Building wheel for yi-yiddish-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yi-yiddish-sm: filename=yi_yiddish_sm-0.0.1-py3-none-any.whl size=7832162 sha256=3d8f92f4d6a684a36a442ac30f2f66810a23a6df887e35970c10a80469580621\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/bc/62/9998d02ad7ee2fbf9ddb7eaeea603e1b3d81ddcb68e635cb90\n",
            "Successfully built yi-yiddish-sm\n",
            "Installing collected packages: yi-yiddish-sm\n",
            "Successfully installed yi-yiddish-sm-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To import the model, use the language code (in this case 'yi' followed by the name (\"yiddish_sm\")) \n",
        "# You don't need the version information: '-0.0.1'\n",
        "#So ✔ Successfully created package 'yi_yiddish_sm-0.0.1'\n",
        "# Becomes yi_yiddish_sm \n",
        "import spacy \n",
        "nlp = spacy.load('yi_yiddish_sm')\n",
        "doc = nlp(\"איך האב א היים אין ישראל\")\n",
        "for token in doc:\n",
        "  print(token.text,token.pos_)\n",
        "for ent in doc.ents:\n",
        "  print(ent)"
      ],
      "metadata": {
        "id": "0otKujKlcNzn",
        "outputId": "158b0345-fa56-48c7-d8c1-18a99baef765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "id": "0otKujKlcNzn",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".lang.{lang}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spacy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy.lang.yi'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-36c1e8c2500f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Becomes yi_yiddish_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yi_yiddish_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"איך האב א היים אין ישראל\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m     51\u001b[0m     return util.load_model(\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"blank:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \"\"\"\n\u001b[1;32m    452\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/yi_yiddish_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m     )\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_config\u001b[0;34m(config, vocab, disable, exclude, auto_fill, validate)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;31m# This will automatically handle all codes registered via the languages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;31m# registry, including custom subclasses provided via entry points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     \u001b[0mlang_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lang\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m     nlp = lang_cls.from_config(\n\u001b[1;32m    526\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".lang.{lang}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spacy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE048\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0mset_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: [E048] Can't import language yi or any matching language from spacy.lang: No module named 'spacy.lang.yi'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "New Language Training (Colab).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}